import os
import json
import re
import ollama
import numpy as np
from tqdm import tqdm

# --- CONFIGURACI√ìN PARA R√âPLICA CIENT√çFICA ---
DATASET_DIR = "dataset_ground_truth_v2"
METADATA_FILE = os.path.join(DATASET_DIR, "dataset_metadata_v2.json")
OUTPUT_FILE = "embeddings_typescript_replica.json"

# LISTA DE MODELOS PARA COMPARAR ARQUITECTURAS
# Usamos modelos que SOPORTAN embeddings en Ollama
MODELS = [
    # 1. Modelos de embedding dedicados (RECOMENDADOS)
    "nomic-embed-text:latest",   # ~137M params, bueno para c√≥digo
    "bge-m3:latest",             # Multiling√ºe, buena calidad
    # "all-minilm:latest",       # Alternativa ligera
    # "mxbai-embed-large:latest" # Alta calidad
]

# NOTA: deepseek-coder y qwen2.5-coder son modelos GENERATIVOS (LLM)
# NO soportan embeddings nativamente en Ollama. Si necesitas usarlos,
# tendr√≠as que extraer embeddings de la √∫ltima capa oculta manualmente.

# Configuraci√≥n de Ventana Deslizante (Vital para archivos grandes)
CHUNK_SIZE_CHARS = 4000  # Tama√±o de cada chunk
OVERLAP_CHARS = 200      # Solapamiento entre chunks      

def get_sliding_window_embedding(model_name, text):
    """
    Genera embeddings usando ventana deslizante y promedio.
    Soporta tanto modelos de Embedding puros como modelos de Chat (Qwen/Deepseek).
    """
    # Dividir en chunks
    chunks = []
    start = 0
    
    # Seguridad: Si el texto es vac√≠o
    if not text.strip(): return []

    # Si es peque√±o, un solo chunk
    if len(text) <= CHUNK_SIZE_CHARS:
        chunks.append(text)
    else:
        # Loop de ventana deslizante
        while start < len(text):
            end = min(start + CHUNK_SIZE_CHARS, len(text))
            chunk = text[start:end]
            chunks.append(chunk)
            if end == len(text):
                break
            start += CHUNK_SIZE_CHARS - OVERLAP_CHARS
    
    vectors = []
    for chunk in chunks:
        try:
            # Ollama permite pedir embeddings a modelos de chat (qwen/deepseek)
            # El endpoint es el mismo.
            resp = ollama.embeddings(model=model_name, prompt=chunk)
            emb = resp.get("embedding")
            
            if emb:
                vectors.append(emb)
            else:
                # A veces los modelos de chat devuelven vac√≠o si el prompt es raro
                print(f"   ‚ö†Ô∏è Vector vac√≠o recibido de {model_name}")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error en chunk con {model_name}: {e}")

    if not vectors:
        return []
    
    # Promedio de los vectores (Mean Pooling) para tener 1 solo vector por archivo
    return np.mean(vectors, axis=0).tolist()

def clean_typescript_code(code):
    """Limpieza est√°ndar (sin ser destructiva)"""
    code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)  # Quitar bloques /* */
    code = re.sub(r'//.*', '', code)  # Quitar comentarios de l√≠nea
    lines = [line.strip() for line in code.split('\n') if line.strip()]
    return '\n'.join(lines)


def load_dataset_from_metadata():
    """Carga los archivos desde el metadata JSON del dataset"""
    if not os.path.exists(METADATA_FILE):
        print(f"‚ùå No se encontr√≥ {METADATA_FILE}")
        return []
    
    with open(METADATA_FILE, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    
    files = []
    samples = metadata.get("samples", [])
    
    for sample in samples:
        # Convertir rutas Windows a Linux y hacerlas relativas al repo
        file_path = sample.get("file_path", "")
        # Quitar backslashes de Windows y el primer separador
        file_path = file_path.replace("\\", "/").lstrip("/")
        
        # La ruta completa es: repos/<file_path> o dataset_ground_truth_v2/<pattern>/<archivo>
        # Verificamos si existe en repos/
        full_path_repos = os.path.join("repos", file_path)
        
        # Buscar tambi√©n en el directorio del dataset por patr√≥n
        pattern_name = sample.get("pattern_name", "Unknown")
        filename = os.path.basename(file_path)
        full_path_dataset = os.path.join(DATASET_DIR, pattern_name, filename)
        
        # Intentar encontrar el archivo
        if os.path.exists(full_path_repos):
            actual_path = full_path_repos
        elif os.path.exists(full_path_dataset):
            actual_path = full_path_dataset
        else:
            # B√∫squeda alternativa en el directorio del patr√≥n
            pattern_dir = os.path.join(DATASET_DIR, pattern_name)
            if os.path.exists(pattern_dir):
                # Buscar archivo que contenga el nombre
                for f_name in os.listdir(pattern_dir):
                    if f_name.endswith(('.ts', '.tsx')):
                        actual_path = os.path.join(pattern_dir, f_name)
                        break
                else:
                    continue
            else:
                continue
        
        files.append({
            "path": actual_path,
            "filename": filename,
            "label": pattern_name,
            "confidence": sample.get("confidence_score", 0),
            "source_repo": sample.get("source_repo", "unknown")
        })
    
    return files


def get_files_from_directory():
    """Alternativa: buscar archivos directamente en el directorio"""
    files = []
    if os.path.exists(DATASET_DIR):
        for root, dirs, filenames in os.walk(DATASET_DIR):
            for filename in filenames:
                if filename.endswith(".ts") or filename.endswith(".tsx"):
                    files.append({
                        "path": os.path.join(root, filename),
                        "filename": filename,
                        "label": os.path.basename(root),
                        "confidence": 1.0,
                        "source_repo": "local"
                    })
    return files


def main():
    print("--- REPLICACI√ìN PANDEY ET AL. (2025) - STEP 1: EMBEDDINGS ---")
    
    # Verificar modelos antes de empezar
    try:
        installed = [m['name'] for m in ollama.list()['models']]
        print(f"Modelos instalados: {installed}")
        missing_models = []
        for m in MODELS:
            base_name = m.split(':')[0]
            if not any(base_name in ins for ins in installed):
                missing_models.append(m)
                print(f"‚ùå ADVERTENCIA: No veo '{m}' instalado. Ejecuta: ollama pull {m}")
        
        if missing_models:
            print(f"\n‚ö†Ô∏è  Faltan {len(missing_models)} modelos. ¬øContinuar con los disponibles? (s/n)")
            # Comentar esto si quieres que contin√∫e autom√°ticamente
            # resp = input()
            # if resp.lower() != 's':
            #     return
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo verificar modelos de Ollama: {e}")

    # Cargar archivos - intentar primero desde metadata, si falla usar directorio
    files = get_files_from_directory()  # M√°s simple y directo
    
    if not files:
        print("‚ùå No se encontraron archivos TypeScript en el dataset")
        return
    
    print(f"üìÇ Archivos a procesar: {len(files)}")
    
    # Mostrar distribuci√≥n de patrones
    pattern_counts = {}
    for f in files:
        pattern_counts[f["label"]] = pattern_counts.get(f["label"], 0) + 1
    print(f"üìä Distribuci√≥n de patrones: {pattern_counts}")
    
    final_data = []

    # Iterar por archivo
    for f in tqdm(files, desc="Procesando Archivos"):
        try:
            with open(f["path"], "r", encoding="utf-8", errors="ignore") as fr:
                content = fr.read()
            
            cleaned = clean_typescript_code(content)
            if not cleaned:
                continue

            file_entry = {
                "filename": f["filename"],
                "label": f["label"],
                "source_repo": f.get("source_repo", "unknown"),
                "confidence": f.get("confidence", 1.0),
                "code_length": len(cleaned),
                "vectors": {}
            }

            # Generar vector con CADA modelo para este archivo
            for model_name in MODELS:
                vec = get_sliding_window_embedding(model_name, cleaned)
                if vec:
                    file_entry["vectors"][model_name] = {
                        "embedding": vec,
                        "dimension": len(vec)
                    }
            
            # Solo guardamos si se gener√≥ al menos un vector
            if file_entry["vectors"]:
                final_data.append(file_entry)

        except Exception as e:
            print(f"Error archivo {f['filename']}: {e}")

    # Guardar resultado
    print(f"\nüíæ Guardando {OUTPUT_FILE}...")
    
    output_data = {
        "metadata": {
            "total_files": len(final_data),
            "models_used": MODELS,
            "chunk_size": CHUNK_SIZE_CHARS,
            "overlap": OVERLAP_CHARS,
            "pattern_distribution": pattern_counts
        },
        "embeddings": final_data
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
        json.dump(output_data, out, indent=2)
    
    print(f"‚úÖ Proceso terminado. {len(final_data)} archivos procesados.")
    print("   Listo para fase de Clasificaci√≥n (k-NN).")

if __name__ == "__main__":
    main()